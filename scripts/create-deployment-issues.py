#!/usr/bin/env python3
"""
Create GitHub issues for deployment problems encountered
"""

import requests
import json
import os
from datetime import datetime

# GitHub configuration
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')
REPO_OWNER = 'markwoitaszek'
REPO_NAME = 'llm-multimodal-stack'
API_BASE = 'https://api.github.com'

def create_issue(title, body, labels=None, assignees=None):
    """Create a GitHub issue"""
    if not GITHUB_TOKEN:
        print("âŒ GITHUB_TOKEN environment variable not set")
        print("ğŸ”§ Please set it with: export GITHUB_TOKEN=your_github_token")
        return None
    
    url = f"{API_BASE}/repos/{REPO_OWNER}/{REPO_NAME}/issues"
    
    headers = {
        'Authorization': f'token {GITHUB_TOKEN}',
        'Accept': 'application/vnd.github.v3+json',
        'Content-Type': 'application/json'
    }
    
    data = {
        'title': title,
        'body': body,
        'labels': labels or [],
        'assignees': assignees or []
    }
    
    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        
        issue = response.json()
        print(f"âœ… Created issue #{issue['number']}: {title}")
        return issue
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Failed to create issue '{title}': {e}")
        if hasattr(e, 'response') and e.response:
            print(f"Response: {e.response.text}")
        return None

def main():
    """Create all deployment issues"""
    print("ğŸ› Creating GitHub issues for deployment problems...")
    print(f"ğŸ“ Repository: {REPO_OWNER}/{REPO_NAME}")
    print()
    
    issues = [
        {
            'title': '[BUG] LiteLLM database authentication failure during deployment',
            'body': '''## ğŸ› Bug Description

LiteLLM service fails to start due to PostgreSQL authentication errors during deployment.

## ğŸ” Component
LiteLLM Router

## ğŸš¨ Severity
High - Major functionality is broken

## ğŸ“‹ Steps to Reproduce
1. Run `./scripts/setup.sh`
2. Run `docker-compose up -d litellm`
3. Check logs with `docker-compose logs litellm`

## ğŸ–¥ï¸ Environment
- Server: seismic (RTX 3090)
- OS: Ubuntu with Docker
- Branch: develop
- Commit: 89aa660

## ğŸ“ Error Details

```
Error: P1000: Authentication failed against database server at `postgres`, 
the provided database credentials for `postgres` are not valid.
```

## ğŸ” Analysis
- PostgreSQL is running and healthy
- Generated passwords in .env file appear correct
- LiteLLM's Prisma client cannot connect to PostgreSQL
- Issue may be related to database URL format or timing

## ğŸ¯ Expected Behavior
LiteLLM should connect to PostgreSQL successfully and provide OpenAI-compatible API routing.

## ğŸ”§ Workaround Applied
- Configured OpenWebUI to connect directly to vLLM
- System is functional without LiteLLM router

## ğŸ“š Additional Context
- Part of initial deployment on seismic server
- Core vLLM functionality is working correctly
- Database credentials auto-generated by setup script''',
            'labels': ['bug', 'priority/high', 'component/api', 'deployment']
        },
        
        {
            'title': '[BUG] Multimodal Worker CUDA base image compatibility issue',
            'body': '''## ğŸ› Bug Description

Multimodal Worker service fails to build due to unavailable CUDA base image during deployment.

## ğŸ” Component
Multimodal Worker

## ğŸš¨ Severity
Medium - Some functionality is broken

## ğŸ“‹ Steps to Reproduce
1. Run `docker-compose up -d multimodal-worker`
2. Observe build failure

## ğŸ–¥ï¸ Environment
- Server: seismic (RTX 3090)
- CUDA Version: 13.0
- Docker version: 27.5.1
- Branch: develop
- Commit: 89aa660

## ğŸ“ Error Details

```
ERROR: failed to solve: nvidia/cuda:11.8-devel-ubuntu22.04: failed to resolve source metadata 
for docker.io/nvidia/cuda:11.8-devel-ubuntu22.04: 
docker.io/nvidia/cuda:11.8-devel-ubuntu22.04: not found
```

## ğŸ” Analysis
- Server has CUDA 13.0 but Dockerfile specifies CUDA 11.8
- CUDA 11.8 base image may not be available
- Need to update to compatible CUDA version

## ğŸ¯ Expected Behavior
Multimodal Worker should build successfully and provide image/video/text processing capabilities.

## ğŸ”§ Suggested Fix
Update Dockerfile to use compatible CUDA base image:
```dockerfile
FROM nvidia/cuda:12.0-devel-ubuntu22.04
# or
FROM nvidia/cuda:12.2-devel-ubuntu22.04
```

## ğŸ“š Additional Context
- Affects CLIP, BLIP-2, Whisper model loading
- Prevents multimodal processing capabilities
- Core LLM functionality works without this service''',
            'labels': ['bug', 'priority/medium', 'component/worker', 'docker', 'gpu']
        },
        
        {
            'title': '[TASK] Fix service health check endpoints and timing',
            'body': '''## ğŸ“‹ Task Description

Several services are showing as "unhealthy" in Docker health checks despite being functional.

## ğŸ”§ Task Type
Maintenance

## ğŸš¨ Priority
Medium - Normal priority

## ğŸ“‹ Services Affected
- Qdrant: Health check using wrong endpoint
- MinIO: Health check timing issues
- vLLM: Health check timing for model loading

## ğŸ” Current Issues

### Qdrant
- Health check uses `/health` but service responds at `/`
- Service is functional but shows as unhealthy

### MinIO
- Health check may be too aggressive during startup
- Service is functional but shows as unhealthy

### vLLM
- Health check doesn't account for model loading time
- Should wait longer for initial model download

## âœ… Acceptance Criteria
- [ ] All functional services show as healthy in `docker-compose ps`
- [ ] Health check script passes for all working services
- [ ] Health check timeouts appropriate for service startup times
- [ ] Health check endpoints correct for each service

## ğŸ”§ Implementation Notes
- Update health check endpoints in docker-compose.yml
- Adjust health check intervals and timeouts
- Update health-check.sh script with correct endpoints
- Consider startup grace periods for model loading

## ğŸ“š References
- Qdrant API documentation
- MinIO health check documentation
- vLLM startup behavior''',
            'labels': ['task', 'priority/medium', 'component/docker', 'monitoring']
        },
        
        {
            'title': '[TASK] Resolve port conflicts with existing services',
            'body': '''## ğŸ“‹ Task Description

Port conflicts detected during deployment with existing services on seismic server.

## ğŸ”§ Task Type
Configuration

## ğŸš¨ Priority
Medium - Normal priority

## ğŸ” Conflicts Identified

### Port 9001
- **Conflict**: Portainer Agent using port 9001
- **Our Service**: MinIO Console
- **Resolution Applied**: Changed MinIO console to port 9002

### Port 3000
- **Potential Conflict**: Grafana was using port 3000
- **Our Service**: OpenWebUI
- **Status**: Currently available, monitoring needed

## âœ… Acceptance Criteria
- [ ] Document all port assignments and conflicts
- [ ] Update documentation with correct port numbers
- [ ] Ensure no conflicts with existing seismic server services
- [ ] Update README and documentation with correct URLs

## ğŸ”§ Implementation
- [x] MinIO console moved to port 9002
- [ ] Update all documentation references
- [ ] Add port conflict detection to setup script
- [ ] Document seismic server port usage

## ğŸ“š Current Port Assignments
- PostgreSQL: 5432
- Qdrant: 6333-6334
- vLLM: 8000
- Multimodal Worker: 8001 (planned)
- Retrieval Proxy: 8002 (planned)
- OpenWebUI: 3000
- LiteLLM: 4000 (planned)
- MinIO: 9000, 9002 (console)

## ğŸ“ Additional Notes
- Seismic server runs Portainer, Grafana, Prometheus
- Need coordination with existing monitoring stack
- Consider using different port ranges for multimodal stack''',
            'labels': ['task', 'priority/medium', 'component/docker', 'configuration']
        },
        
        {
            'title': '[ENHANCEMENT] Complete multimodal worker deployment',
            'body': '''## âœ¨ Feature Summary

Complete the deployment of multimodal worker service for image, video, and text processing capabilities.

## ğŸ“‹ Feature Category
Infrastructure/DevOps

## ğŸš¨ Priority
High - Significantly improves user experience

## ğŸ¯ Problem Statement

Currently only LLM inference is working. The multimodal capabilities (CLIP, BLIP-2, Whisper) are not available due to:
- CUDA base image compatibility issues
- Service build failures
- Missing GPU optimization for multimodal models

## ğŸ’¡ Proposed Solution

1. **Fix CUDA Base Image**
   - Update to CUDA 12.x compatible base image
   - Test with seismic server's CUDA 13.0 environment

2. **Optimize GPU Memory Allocation**
   - vLLM currently uses 16GB of 24GB available
   - Reserve remaining 8GB for multimodal models
   - Implement proper GPU memory management

3. **Service Dependencies**
   - Ensure proper startup order
   - Add health checks for model loading
   - Configure inter-service communication

## âœ… Acceptance Criteria
- [ ] Multimodal worker builds successfully
- [ ] CLIP image embeddings working
- [ ] BLIP-2 image captioning working  
- [ ] Whisper audio transcription working
- [ ] GPU memory properly allocated between services
- [ ] Health checks pass for multimodal worker
- [ ] API endpoints respond correctly

## ğŸ® Technical Considerations

### GPU Memory Management
- Current: vLLM using 16GB/24GB
- Target: vLLM 14GB, Multimodal Worker 8GB, Buffer 2GB

### Model Loading Strategy
- Pre-download models during build
- Implement model caching
- Optimize startup time

### Performance Impact
- Monitor GPU utilization
- Benchmark processing speeds
- Ensure no interference between services

## ğŸ“š Resources
- NVIDIA CUDA compatibility matrix
- vLLM GPU memory optimization docs
- PyTorch CUDA memory management''',
            'labels': ['enhancement', 'priority/high', 'component/worker', 'gpu']
        }
    ]
    
    created_issues = []
    for issue in issues:
        result = create_issue(
            title=issue['title'],
            body=issue['body'],
            labels=issue['labels']
        )
        if result:
            created_issues.append(result)
    
    print()
    print(f"ğŸ‰ Successfully created {len(created_issues)} issues!")
    print()
    print("ğŸ“‹ Created Issues:")
    for issue in created_issues:
        print(f"#{issue['number']}: {issue['title']}")
        print(f"   ğŸ”— {issue['html_url']}")
    
    print()
    print("ğŸ”— View all issues: https://github.com/markwoitaszek/llm-multimodal-stack/issues")

if __name__ == "__main__":
    main()
