# ğŸš€ Quick Start Guide

**Get your Multimodal LLM Stack running in under 10 minutes!**

## ğŸ¯ What You'll Get

After this guide, you'll have:
- ğŸ¤– **ChatGPT-like AI interface** running locally
- ğŸ”Œ **OpenAI-compatible API** for development
- ğŸ’¾ **Vector database** for intelligent search
- ğŸ“Š **Monitoring dashboards** for system health

## âš¡ One-Command Setup

```bash
# Download and run the stack
curl -fsSL https://raw.githubusercontent.com/markwoitaszek/llm-multimodal-stack/main/scripts/quick-deploy.sh | bash
```

## ğŸ“‹ Manual Setup (5 Steps)

### Step 1: Prerequisites Check âœ…

**Required:**
- Docker & Docker Compose
- Python 3.13+
- 8GB+ free disk space
- Internet connection

**Optional (for GPU acceleration):**
- NVIDIA GPU with 8GB+ VRAM

### Step 2: Setup Secrets Management ğŸ”

**Phase-6A includes production-grade secrets management:**

```bash
# Generate secure secrets and environment files
python3 setup_secrets.py
```

This automatically creates:
- âœ… **21 secure secrets** (passwords, API keys, etc.)
- âœ… **Encrypted storage** with proper permissions
- âœ… **Environment-specific** configurations
- âœ… **Docker Compose** integration with environment variables
- âœ… **Kubernetes secrets** templates
- âœ… **Configurable service ports and hosts**
- âœ… **Flexible service URLs for different environments**

**Quick Check:**
```bash
# Check if you have Docker
docker --version

# Check if you have GPU (optional)
nvidia-smi
```

### Step 2: Download the Stack ğŸ“¥

```bash
# Clone the repository
git clone https://github.com/markwoitaszek/llm-multimodal-stack.git
cd llm-multimodal-stack
```

### Step 3: Automatic Setup ğŸ”§

```bash
# This handles everything automatically:
# - Generates secure passwords
# - Configures GPU (if available)
# - Sets up storage paths
# - Checks for conflicts
./scripts/setup.sh
```

### Step 4: Start the AI Stack ğŸš€

```bash
# Start all services
docker-compose up -d

# Wait for services to load (first time takes 2-3 minutes)
echo "â³ Starting services... This may take a few minutes on first run."
```

### Step 5: Access Your AI System ğŸŒ

```bash
# Check if everything is working
./scripts/comprehensive-health-check.sh

# Open your AI interface
open http://localhost:3030
```

## ğŸ‰ You're Done!

**Your AI system is now running at:**
- ğŸ¤– **Chat Interface**: http://localhost:3030
- ğŸ”Œ **API Endpoint**: http://localhost:8000/v1
- ğŸ“Š **System Health**: Run `./scripts/stack-manager.sh status`

## ğŸ†˜ Troubleshooting

### â“ Common Issues

**ğŸ³ Docker not found:**
```bash
# Install Docker (Ubuntu/Debian)
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER
# Log out and back in
```

**ğŸ® GPU not detected:**
```bash
# Install NVIDIA Docker support
sudo apt-get update
sudo apt-get install nvidia-container-toolkit
sudo systemctl restart docker
```

**ğŸ”Œ Port conflicts:**
```bash
# The setup script will detect and resolve most conflicts
# Check what's using ports:
ss -tulpn | grep -E ":(3030|8000|6333|9000)"
```

**ğŸŒ Slow startup:**
```bash
# First time downloads models (~1GB), this is normal
# Monitor progress:
docker-compose logs vllm
```

### ğŸ†˜ Get Help

**Quick Diagnostics:**
```bash
# Run comprehensive health check
./scripts/comprehensive-health-check.sh

# Check service logs
./scripts/stack-manager.sh logs

# Get system info
./scripts/stack-manager.sh status
```

**Support Resources:**
- ğŸ“š [Full Documentation](docs/)
- ğŸ› [Report Issues](https://github.com/markwoitaszek/llm-multimodal-stack/issues)
- ğŸ’¬ [Community Discussions](https://github.com/markwoitaszek/llm-multimodal-stack/discussions)

## ğŸ¯ Next Steps

**Try These Features:**
1. **Chat with AI**: Ask questions, get summaries, creative writing
2. **Upload Images**: Drag & drop images for AI analysis (coming soon)
3. **API Development**: Use the OpenAI-compatible endpoint in your code
4. **Monitor System**: Check GPU usage and system health

**Example API Usage:**
```python
import openai

# Configure for your local stack
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "dummy-key"  # Not required for local use

# Chat with your AI
response = openai.ChatCompletion.create(
    model="microsoft/DialoGPT-medium",
    messages=[{"role": "user", "content": "Explain machine learning"}]
)

print(response.choices[0].message.content)
```

## ğŸš€ Advanced Usage

**Management Commands:**
```bash
# Use the unified management tool
./scripts/stack-manager.sh help

# Common commands:
./scripts/stack-manager.sh status     # System status
./scripts/stack-manager.sh restart   # Restart all services
./scripts/stack-manager.sh backup    # Create backup
./scripts/stack-manager.sh update    # Update software
```

**Scale Up:**
```bash
# Add more processing power
docker-compose up -d --scale multimodal-worker=2

# Monitor performance
./scripts/stack-manager.sh benchmark
```

---

**ğŸ‰ Welcome to your personal AI stack!** You now have enterprise-grade AI capabilities running on your own hardware. ğŸš€
