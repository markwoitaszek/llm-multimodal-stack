# Unified Compose Schema for Multimodal LLM Stack
# This schema defines all services, configurations, and environments in one place
# Compose files are generated from this schema using the compose-generator.py script

# version: "3.8"  # Version attribute is obsolete in Docker Compose v2

# Global configuration
config:
  project_name: "multimodal"
  network_name: "multimodal-net"
  default_restart_policy: "unless-stopped"
  
  # Network configuration
  network:
    driver: "bridge"
    ipam:
      driver: "default"
      config:
        - subnet: "172.25.0.0/24"
          gateway: "172.25.0.1"

  # Volume configuration
  volume_config:
    driver: "local"

  # Health check templates
  health_checks:
    standard:
      interval: "30s"
      timeout: "10s"
      retries: 3
    extended:
      interval: "30s"
      timeout: "10s"
      retries: 3
      start_period: "30s"
    slow:
      interval: "60s"
      timeout: "30s"
      retries: 5
      start_period: "180s"

# Service definitions
services:
  # Core Infrastructure Services
  postgres:
    category: "core"
    image: "postgres:16-alpine"
    container_name: "multimodal-postgres"
    ports:
      - "${POSTGRES_PORT}:5432"
    environment:
      - "POSTGRES_DB=${POSTGRES_DB:-multimodal}"
      - "POSTGRES_USER=${POSTGRES_USER}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD}"
    volumes:
      - "postgres_data:/var/lib/postgresql/data"
      - "./sql/init.sql:/docker-entrypoint-initdb.d/init.sql"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      template: "standard"
    volumes_required:
      - "postgres_data"

  redis:
    category: "core"
    image: "redis:7-alpine"
    container_name: "multimodal-redis"
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - "redis_data:/data"
    command: "redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      template: "standard"
    volumes_required:
      - "redis_data"

  qdrant:
    category: "core"
    image: "qdrant/qdrant:v1.12.0"
    container_name: "multimodal-qdrant"
    ports:
      - "${QDRANT_HTTP_PORT:-6333}:${QDRANT_HTTP_PORT:-6333}"
      - "${QDRANT_GRPC_PORT:-6334}:${QDRANT_GRPC_PORT:-6334}"
    volumes:
      - "qdrant_data:/qdrant/storage"
    environment:
      - "QDRANT__SERVICE__HTTP_PORT=${QDRANT_HTTP_PORT:-6333}"
      - "QDRANT__SERVICE__GRPC_PORT=${QDRANT_GRPC_PORT:-6334}"
    healthcheck:
      test: ["CMD-SHELL", "pidof qdrant || exit 1"]
      template: "extended"
    volumes_required:
      - "qdrant_data"

  minio:
    category: "core"
    image: "minio/minio:latest"
    container_name: "multimodal-minio"
    ports:
      - "${MINIO_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9002}:9001"
    environment:
      - "MINIO_ROOT_USER=${MINIO_ROOT_USER}"
      - "MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}"
    volumes:
      - "minio_data:/data"
    command: "server /data --console-address \":9001\""
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MINIO_PORT:-9000}/minio/health/live"]
      interval: "30s"
      timeout: "10s"
      retries: 5
      start_period: "60s"
    volumes_required:
      - "minio_data"

  # Inference Services
  vllm:
    category: "inference"
    image: "vllm/vllm-openai:latest"
    container_name: "multimodal-vllm"
    ports:
      - "${VLLM_PORT:-8000}:8000"
    environment:
      - "CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}"
      - "VLLM_MODEL=${VLLM_MODEL:-microsoft/DialoGPT-medium}"
      - "VLLM_HOST=${VLLM_HOST:-0.0.0.0}"
      - "VLLM_PORT=${VLLM_PORT:-8000}"
    volumes:
      - "vllm_cache:/root/.cache"
      - "./models:/models"
    command: >
      --model ${VLLM_MODEL:-microsoft/DialoGPT-medium}
      --host ${VLLM_HOST:-0.0.0.0}
      --port ${VLLM_PORT:-8000}
      --gpu-memory-utilization 0.8
      --max-model-len 1024
      --dtype auto
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:${VLLM_PORT:-8000}/v1/models', timeout=10)\""]
      template: "slow"
    volumes_required:
      - "vllm_cache"

  litellm:
    category: "inference"
    image: "ghcr.io/berriai/litellm:main-latest"
    container_name: "multimodal-litellm"
    ports:
      - "${LITELLM_PORT}:4000"
    environment:
      - "LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}"
      - "LITELLM_SALT_KEY=${LITELLM_SALT_KEY}"
      - "DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}"
      - "POSTGRES_DB=${POSTGRES_DB:-multimodal}"
      - "POSTGRES_USER=${POSTGRES_USER}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD}"
    volumes:
      - "./configs/litellm_simple.yaml:/app/config.yaml"
    command: ["--config", "/app/config.yaml", "--port", "${LITELLM_PORT}", "--num_workers", "1"]
    depends_on:
      - "vllm"
      - "postgres"
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:${LITELLM_PORT}/', timeout=5)\""]
      template: "standard"

  # Multimodal Services
  multimodal-worker:
    category: "multimodal"
    build:
      context: "./services/multimodal-worker"
      dockerfile: "Dockerfile"
    container_name: "multimodal-worker"
    ports:
      - "${MULTIMODAL_WORKER_PORT:-8001}:8001"
    environment:
      - "CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}"
      - "QDRANT_HOST=${QDRANT_HOST:-qdrant}"
      - "QDRANT_PORT=${QDRANT_HTTP_PORT:-6333}"
      - "POSTGRES_HOST=${POSTGRES_HOST}"
      - "POSTGRES_PORT=${POSTGRES_PORT}"
      - "POSTGRES_DB=${POSTGRES_DB:-multimodal}"
      - "POSTGRES_USER=${POSTGRES_USER}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD}"
      - "MINIO_ENDPOINT=${MINIO_ENDPOINT:-minio:9000}"
      - "MINIO_ACCESS_KEY=${MINIO_ROOT_USER}"
      - "MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD}"
      - "REDIS_HOST=${REDIS_HOST:-redis}"
      - "REDIS_PORT=${REDIS_PORT:-6379}"
      - "REDIS_DB=0"
    volumes:
      - "multimodal_cache:/app/cache"
      - "/tmp:/tmp"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - "qdrant"
      - "postgres"
      - "minio"
      - "redis"
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:${MULTIMODAL_WORKER_PORT:-8001}/health', timeout=5)\""]
      template: "standard"
    volumes_required:
      - "multimodal_cache"

  retrieval-proxy:
    category: "multimodal"
    build:
      context: "./services/retrieval-proxy"
      dockerfile: "Dockerfile"
    container_name: "multimodal-retrieval-proxy"
    ports:
      - "${RETRIEVAL_PROXY_PORT:-8002}:8002"
    environment:
      - "QDRANT_HOST=${QDRANT_HOST:-qdrant}"
      - "QDRANT_PORT=${QDRANT_HTTP_PORT:-6333}"
      - "POSTGRES_HOST=${POSTGRES_HOST}"
      - "POSTGRES_PORT=${POSTGRES_PORT}"
      - "POSTGRES_DB=${POSTGRES_DB:-multimodal}"
      - "POSTGRES_USER=${POSTGRES_USER}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD}"
      - "MINIO_ENDPOINT=${MINIO_ENDPOINT:-minio:9000}"
      - "MINIO_ACCESS_KEY=${MINIO_ROOT_USER}"
      - "MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD}"
      - "MULTIMODAL_WORKER_URL=${MULTIMODAL_WORKER_URL:-http://multimodal-worker:8001}"
      - "REDIS_HOST=${REDIS_HOST:-redis}"
      - "REDIS_PORT=${REDIS_PORT:-6379}"
      - "REDIS_DB=1"
    depends_on:
      - "multimodal-worker"
      - "qdrant"
      - "postgres"
      - "redis"
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:${RETRIEVAL_PROXY_PORT:-8002}/health', timeout=5)\""]
      template: "standard"

  # AI Services
  ai-agents:
    category: "ai-services"
    build:
      context: "./services/ai-agents"
      dockerfile: "Dockerfile"
    container_name: "multimodal-ai-agents"
    ports:
      - "${AI_AGENTS_PORT:-8003}:8003"
    environment:
      - "POSTGRES_HOST=${POSTGRES_HOST}"
      - "POSTGRES_PORT=${POSTGRES_PORT}"
      - "POSTGRES_DB=${POSTGRES_DB:-multimodal}"
      - "POSTGRES_USER=${POSTGRES_USER}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD}"
      - "QDRANT_HOST=${QDRANT_HOST:-qdrant}"
      - "QDRANT_PORT=${QDRANT_HTTP_PORT:-6333}"
      - "LLM_BASE_URL=${LLM_BASE_URL:-http://vllm:8000/v1}"
      - "LLM_MODEL=${VLLM_MODEL:-microsoft/DialoGPT-medium}"
      - "MULTIMODAL_WORKER_URL=${MULTIMODAL_WORKER_URL:-http://multimodal-worker:8001}"
      - "RETRIEVAL_PROXY_URL=${RETRIEVAL_PROXY_URL:-http://multimodal-retrieval-proxy:8002}"
      - "SEARCH_ENGINE_URL=${SEARCH_ENGINE_URL:-http://search-engine:8004}"
      - "MEMORY_SYSTEM_URL=${MEMORY_SYSTEM_URL:-http://memory-system:8005}"
      - "USER_MANAGEMENT_URL=${USER_MANAGEMENT_URL:-http://user-management:8006}"
      - "REDIS_HOST=${REDIS_HOST:-redis}"
      - "REDIS_PORT=${REDIS_PORT:-6379}"
      - "REDIS_DB=2"
    depends_on:
      - "postgres"
      - "qdrant"
      - "redis"
      - "vllm"
      - "multimodal-worker"
      - "retrieval-proxy"
      - "search-engine"
      - "memory-system"
      - "user-management"
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:${AI_AGENTS_PORT:-8003}/health', timeout=5)\""]
      template: "extended"
    profiles:
      - "services"
      - "agents"

  memory-system:
    category: "ai-services"
    build:
      context: "./services/memory-system"
      dockerfile: "Dockerfile"
    container_name: "multimodal-memory-system"
    ports:
      - "${MEMORY_SYSTEM_PORT:-8005}:8005"
    environment:
      - "POSTGRES_HOST=${POSTGRES_HOST}"
      - "POSTGRES_PORT=${POSTGRES_PORT}"
      - "POSTGRES_DB=${POSTGRES_DB:-multimodal}"
      - "POSTGRES_USER=${POSTGRES_USER}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD}"
      - "REDIS_HOST=${REDIS_HOST:-redis}"
      - "REDIS_PORT=${REDIS_PORT:-6379}"
      - "REDIS_DB=4"
    depends_on:
      - "postgres"
      - "redis"
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:${MEMORY_SYSTEM_PORT:-8005}/health', timeout=5)\""]
      template: "extended"
    profiles:
      - "services"
      - "memory"

  search-engine:
    category: "ai-services"
    build:
      context: "./services/search-engine"
      dockerfile: "Dockerfile"
    container_name: "multimodal-search-engine"
    ports:
      - "${SEARCH_ENGINE_PORT:-8004}:8004"
    environment:
      - "POSTGRES_HOST=${POSTGRES_HOST}"
      - "POSTGRES_PORT=${POSTGRES_PORT}"
      - "POSTGRES_DB=${POSTGRES_DB:-multimodal}"
      - "POSTGRES_USER=${POSTGRES_USER}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD}"
      - "QDRANT_HOST=${QDRANT_HOST:-qdrant}"
      - "QDRANT_PORT=${QDRANT_HTTP_PORT:-6333}"
      - "REDIS_HOST=${REDIS_HOST:-redis}"
      - "REDIS_PORT=${REDIS_PORT:-6379}"
      - "REDIS_DB=3"
      - "MULTIMODAL_WORKER_URL=${MULTIMODAL_WORKER_URL:-http://multimodal-worker:8001}"
      - "RETRIEVAL_PROXY_URL=${RETRIEVAL_PROXY_URL:-http://multimodal-retrieval-proxy:8002}"
    depends_on:
      - "postgres"
      - "qdrant"
      - "redis"
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:${SEARCH_ENGINE_PORT:-8004}/health', timeout=5)\""]
      template: "extended"
    profiles:
      - "services"
      - "search"

  user-management:
    category: "ai-services"
    build:
      context: "./services/user-management"
      dockerfile: "Dockerfile"
    container_name: "multimodal-user-management"
    ports:
      - "${USER_MANAGEMENT_PORT:-8006}:8006"
    environment:
      - "POSTGRES_HOST=${POSTGRES_HOST}"
      - "POSTGRES_PORT=${POSTGRES_PORT}"
      - "POSTGRES_DB=${POSTGRES_DB:-multimodal}"
      - "POSTGRES_USER=${POSTGRES_USER}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD}"
      - "REDIS_HOST=${REDIS_HOST:-redis}"
      - "REDIS_PORT=${REDIS_PORT:-6379}"
      - "REDIS_DB=5"
      - "JWT_SECRET_KEY=${JWT_SECRET_KEY}"
      - "DEBUG=${DEBUG:-false}"
    depends_on:
      - "postgres"
      - "redis"
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:${USER_MANAGEMENT_PORT:-8006}/health', timeout=5)\""]
      template: "extended"
    profiles:
      - "services"
      - "auth"

  # UI Services
  openwebui:
    category: "ui"
    image: "ghcr.io/open-webui/open-webui:main"
    container_name: "multimodal-openwebui"
    ports:
      - "${OPENWEBUI_PORT:-3030}:8080"
    environment:
      - "OPENAI_API_BASE_URL=${OPENAI_API_BASE_URL:-http://vllm:8000/v1}"
      - "OPENAI_API_KEY=${VLLM_API_KEY}"
      - "WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}"
    volumes:
      - "openwebui_data:/app/backend/data"
    depends_on:
      - "vllm"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      template: "extended"
    volumes_required:
      - "openwebui_data"
    profiles:
      - "monitoring"
      - "webui"

  ai-agents-web:
    category: "ui"
    build:
      context: "."
      dockerfile: "./services/ai-agents/web/Dockerfile"
    container_name: "multimodal-ai-agents-web"
    ports:
      - "3001:3000"
    volumes:
      - "./docs:/usr/share/nginx/html/docs:ro"
    depends_on:
      - "ai-agents"
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:3000/health', timeout=5)\""]
      template: "extended"
    profiles:
      - "services"
      - "web"

  # Workflow Services
  n8n:
    category: "workflow"
    image: "n8nio/n8n:latest"
    container_name: "multimodal-n8n"
    ports:
      - "${N8N_PORT:-5678}:5678"
    environment:
      - "N8N_BASIC_AUTH_ACTIVE=true"
      - "N8N_BASIC_AUTH_USER=admin"
      - "N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD}"
      - "N8N_HOST=localhost"
      - "N8N_PORT=5678"
      - "N8N_PROTOCOL=https"
      - "WEBHOOK_URL=https://localhost:5678"
      - "GENERIC_TIMEZONE=UTC"
      - "N8N_METRICS=true"
      - "N8N_LOG_LEVEL=info"
      - "N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}"
      - "DB_SQLITE_POOL_SIZE=5"
      - "N8N_RUNNERS_ENABLED=true"
      - "N8N_BLOCK_ENV_ACCESS_IN_NODE=false"
      - "N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true"
    volumes:
      - "n8n_data:/home/node/.n8n"
      - "./workflows:/home/node/.n8n/workflows:ro"
    depends_on:
      - "postgres"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:${N8N_PORT:-5678}/healthz"]
      template: "extended"
    volumes_required:
      - "n8n_data"
    profiles:
      - "monitoring"
      - "workflow"

  n8n-monitoring:
    category: "workflow"
    build:
      context: "./services/n8n-monitoring"
      dockerfile: "Dockerfile"
    container_name: "multimodal-n8n-monitoring"
    ports:
      - "${N8N_MONITORING_PORT:-8008}:8008"
    environment:
      - "N8N_MONITORING_HOST=${N8N_MONITORING_HOST:-0.0.0.0}"
      - "N8N_MONITORING_PORT=${N8N_MONITORING_PORT:-8008}"
      - "DEBUG=${DEBUG:-false}"
      - "N8N_URL=${N8N_URL:-http://n8n:5678}"
      - "N8N_API_KEY=${N8N_API_KEY:-}"
      - "N8N_WEBHOOK_URL=${N8N_WEBHOOK_URL:-http://n8n-monitoring:8008/webhooks/n8n}"
      - "AI_AGENTS_URL=${AI_AGENTS_URL:-http://ai-agents:8003}"
      - "POSTGRES_HOST=${POSTGRES_HOST}"
      - "POSTGRES_PORT=${POSTGRES_PORT}"
      - "POSTGRES_DB=${POSTGRES_DB:-multimodal}"
      - "POSTGRES_USER=${POSTGRES_USER}"
      - "POSTGRES_PASSWORD=${POSTGRES_PASSWORD}"
      - "REDIS_HOST=${REDIS_HOST:-redis}"
      - "REDIS_PORT=${REDIS_PORT:-6379}"
      - "REDIS_DB=7"
      - "MONITORING_INTERVAL=30"
      - "METRICS_RETENTION_DAYS=90"
      - "MAX_EXECUTION_HISTORY=1000"
      - "ALERT_EMAIL_ENABLED=true"
      - "ALERT_SLACK_ENABLED=true"
      - "ALERT_WEBHOOK_ENABLED=false"
      - "ALERT_THRESHOLD_ERROR_RATE=5.0"
      - "ALERT_THRESHOLD_RESPONSE_TIME=30000"
      - "ALERT_THRESHOLD_FAILURE_COUNT=5"
      - "SMTP_HOST=${SMTP_HOST:-}"
      - "SMTP_PORT=587"
      - "SMTP_USER=${SMTP_USER:-}"
      - "SMTP_PASSWORD=${SMTP_PASSWORD:-}"
      - "SMTP_FROM_EMAIL=${SMTP_FROM_EMAIL:-}"
      - "SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL:-}"
      - "SLACK_CHANNEL=#n8n-monitoring"
      - "WS_MAX_CONNECTIONS=100"
      - "WS_HEARTBEAT_INTERVAL=30"
      - "MAX_CONCURRENT_MONITORS=10"
      - "REQUEST_TIMEOUT=30"
      - "SECRET_KEY=${N8N_MONITORING_SECRET_KEY}"
      - "ALLOWED_ORIGINS=*"
      - "LOG_LEVEL=INFO"
    volumes:
      - "n8n_monitoring_cache:/app/cache"
      - "./services/n8n/workflow-templates:/app/workflow-templates:ro"
    depends_on:
      - "postgres"
      - "redis"
      - "n8n"
      - "ai-agents"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${N8N_MONITORING_PORT:-8008}/health"]
      template: "extended"
    volumes_required:
      - "n8n_monitoring_cache"
    profiles:
      - "n8n-monitoring"
      - "monitoring"

  # Monitoring Services
  elasticsearch:
    category: "monitoring"
    image: "docker.elastic.co/elasticsearch/elasticsearch:8.11.0"
    container_name: "multimodal-elasticsearch"
    environment:
      - "discovery.type=single-node"
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - "xpack.security.enabled=false"
      - "xpack.security.enrollment.enabled=false"
    volumes:
      - "elasticsearch_data:/usr/share/elasticsearch/data"
      - "./configs/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro"
    ports:
      - "9200:9200"
      - "9300:9300"
    deploy:
      resources:
        limits:
          memory: "4G"
          cpus: "2.0"
        reservations:
          memory: "2G"
          cpus: "1.0"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: "30s"
      timeout: "10s"
      retries: 5
    volumes_required:
      - "elasticsearch_data"
    profiles:
      - "elk"
      - "logging"

  logstash:
    category: "monitoring"
    image: "docker.elastic.co/logstash/logstash:8.11.0"
    container_name: "multimodal-logstash"
    environment:
      - "LS_JAVA_OPTS=-Xms1g -Xmx1g"
      - "PIPELINE_WORKERS=4"
      - "PIPELINE_BATCH_SIZE=1000"
      - "PIPELINE_BATCH_DELAY=50"
    volumes:
      - "./configs/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro"
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "/var/log/multimodal:/var/log/multimodal:ro"
    ports:
      - "5044:5044"
      - "514:514/udp"
    depends_on:
      - "elasticsearch"
    deploy:
      resources:
        limits:
          memory: "2G"
          cpus: "1.0"
        reservations:
          memory: "1G"
          cpus: "0.5"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9600/_node/stats || exit 1"]
      interval: "30s"
      timeout: "10s"
      retries: 5
    profiles:
      - "elk"
      - "logging"

  kibana:
    category: "monitoring"
    image: "docker.elastic.co/kibana/kibana:8.11.0"
    container_name: "multimodal-kibana"
    environment:
      - "ELASTICSEARCH_HOSTS=http://elasticsearch:9200"
      - "xpack.security.enabled=false"
    volumes:
      - "./configs/kibana.yml:/usr/share/kibana/config/kibana.yml:ro"
    ports:
      - "5601:5601"
    depends_on:
      - "elasticsearch"
    deploy:
      resources:
        limits:
          memory: "2G"
          cpus: "1.0"
        reservations:
          memory: "1G"
          cpus: "0.5"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: "30s"
      timeout: "10s"
      retries: 5
    profiles:
      - "elk"
      - "logging"

  filebeat:
    category: "monitoring"
    image: "docker.elastic.co/beats/filebeat:8.11.0"
    container_name: "multimodal-filebeat"
    user: "root"
    volumes:
      - "./configs/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro"
      - "/var/lib/docker/containers:/var/lib/docker/containers:ro"
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "/var/log/multimodal:/var/log/multimodal:ro"
    depends_on:
      - "logstash"
    deploy:
      resources:
        limits:
          memory: "512M"
          cpus: "0.5"
        reservations:
          memory: "256M"
          cpus: "0.25"
    profiles:
      - "elk"
      - "logging"

  # Infrastructure Services
  nginx:
    category: "infrastructure"
    image: "nginx:alpine"
    container_name: "multimodal-nginx"
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - "./configs/nginx.conf:/etc/nginx/nginx.conf:ro"
      - "./configs/ssl:/etc/nginx/certs:ro"
    depends_on:
      - "litellm"
      - "multimodal-worker"
      - "retrieval-proxy"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1/health"]
      template: "standard"
    profiles:
      - "production"

# Environment configurations
environments:
  development:
    description: "Development environment with core services only"
    services:
      - "postgres"
      - "redis"
      - "qdrant"
      - "minio"
      - "vllm"
      - "litellm"
      - "multimodal-worker"
      - "retrieval-proxy"
    overrides:
      debug: true
      log_level: "DEBUG"

  staging:
    description: "Staging environment with all services"
    services:
      - "postgres"
      - "redis"
      - "qdrant"
      - "minio"
      - "vllm"
      - "litellm"
      - "multimodal-worker"
      - "retrieval-proxy"
      # TODO: Enable these when Dockerfiles are created:
      # - "ai-agents"
      # - "memory-system"
      # - "search-engine"
      # - "user-management"
      # - "openwebui"
      # - "n8n"
      # - "n8n-monitoring"
      # - "nginx"
    overrides:
      debug: false
      log_level: "INFO"
      vllm:
        image: "vllm/vllm-openai:v0.6.3"
        shm_size: "10gb"
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  device_ids: ['0', '1']
                  capabilities: [gpu]
        environment:
          - "CUDA_VISIBLE_DEVICES=0,1"
          - "NVIDIA_VISIBLE_DEVICES=0,1"
        command: [
          "--model", "microsoft/DialoGPT-small",
          "--host", "0.0.0.0",
          "--port", "8000",
          "--gpu-memory-utilization", "0.8",
          "--max-model-len", "512",
          "--dtype", "auto",
          "--tensor-parallel-size", "2"
        ]
      multimodal-worker:
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]
        environment:
          - "CUDA_VISIBLE_DEVICES=1"
          - "NVIDIA_VISIBLE_DEVICES=1"

  production:
    description: "Production environment with all services and optimizations"
    services:
      - "postgres"
      - "redis"
      - "qdrant"
      - "minio"
      - "vllm"
      - "litellm"
      - "multimodal-worker"
      - "retrieval-proxy"
      # TODO: Enable these when Dockerfiles are created:
      # - "ai-agents"
      # - "memory-system"
      # - "search-engine"
      # - "user-management"
      # - "openwebui"
      # - "n8n"
      # - "n8n-monitoring"
      # - "nginx"
    overrides:
      debug: false
      log_level: "WARN"
      replicas:
        postgres: 1
        redis: 1
        multimodal-worker: 3
        retrieval-proxy: 3
        litellm: 2
      vllm:
        image: "vllm/vllm-openai:v0.6.3"
        shm_size: "10gb"
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  device_ids: ['0', '1']
                  capabilities: [gpu]
        environment:
          - "CUDA_VISIBLE_DEVICES=0,1"
          - "NVIDIA_VISIBLE_DEVICES=0,1"
        command: [
          "--model", "microsoft/DialoGPT-small",
          "--host", "0.0.0.0",
          "--port", "8000",
          "--gpu-memory-utilization", "0.8",
          "--max-model-len", "512",
          "--dtype", "auto",
          "--tensor-parallel-size", "2"
        ]
      multimodal-worker:
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]
        environment:
          - "CUDA_VISIBLE_DEVICES=1"
          - "NVIDIA_VISIBLE_DEVICES=1"
      resources:
        postgres:
          limits:
            memory: "4G"
            cpus: "2.0"
          reservations:
            memory: "2G"
            cpus: "1.0"
        redis:
          limits:
            memory: "1G"
            cpus: "1.0"
          reservations:
            memory: "512M"
            cpus: "0.5"

  gpu:
    description: "GPU-optimized environment"
    services:
      - "postgres"
      - "redis"
      - "qdrant"
      - "minio"
      - "vllm"
      - "litellm"
      - "multimodal-worker"
      - "retrieval-proxy"
    overrides:
      vllm:
        image: "vllm/vllm-openai:v0.6.3"
        shm_size: "10gb"
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  device_ids: ['0', '1']
                  capabilities: [gpu]
        environment:
          - "CUDA_VISIBLE_DEVICES=0,1"
          - "NVIDIA_VISIBLE_DEVICES=0,1"
        command: [
          "--model", "microsoft/DialoGPT-small",
          "--host", "0.0.0.0",
          "--port", "8000",
          "--gpu-memory-utilization", "0.8",
          "--max-model-len", "512",
          "--dtype", "auto",
          "--tensor-parallel-size", "2"
        ]
      multimodal-worker:
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]
        environment:
          - "CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-1}"
          - "NVIDIA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-1}"

  monitoring:
    description: "Monitoring environment with ELK stack"
    services:
      - "postgres"
      - "redis"
      - "qdrant"
      - "minio"
      - "vllm"
      - "litellm"
      - "multimodal-worker"
      - "retrieval-proxy"
      - "openwebui"
      - "n8n"
      - "elasticsearch"
      - "logstash"
      - "kibana"
      - "filebeat"
    overrides:
      debug: false
      log_level: "INFO"

# Volume definitions
volumes:
  postgres_data:
    driver: "local"
  redis_data:
    driver: "local"
  qdrant_data:
    driver: "local"
  minio_data:
    driver: "local"
  vllm_cache:
    driver: "local"
  multimodal_cache:
    driver: "local"
  openwebui_data:
    driver: "local"
  n8n_data:
    driver: "local"
  n8n_monitoring_cache:
    driver: "local"
  elasticsearch_data:
    driver: "local"