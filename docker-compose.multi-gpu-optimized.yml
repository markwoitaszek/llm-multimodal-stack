# Optimized Multi-GPU Configuration with Configurable Memory Allocation
# This configuration allows for model-specific memory allocation

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: multimodal-vllm
    ports:
      - "8000:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0,1
      - NVIDIA_VISIBLE_DEVICES=0,1
      - VLLM_MODEL=${VLLM_MODEL:-microsoft/DialoGPT-large}  # Better model choice
      - VLLM_HOST=0.0.0.0
      - VLLM_PORT=8000
      - VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION:-0.4}  # Configurable memory usage
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-2048}  # Configurable context length
      - VLLM_MAX_NUM_SEQS=${VLLM_MAX_NUM_SEQS:-8}  # Configurable batch size
    volumes:
      - vllm_cache:/root/.cache
      - ./models:/models
    command: [
      "--model", "${VLLM_MODEL:-microsoft/DialoGPT-large}",
      "--host", "0.0.0.0",
      "--port", "8000",
      "--gpu-memory-utilization", "${VLLM_GPU_MEMORY_UTILIZATION:-0.4}",
      "--max-model-len", "${VLLM_MAX_MODEL_LEN:-2048}",
      "--max-num-seqs", "${VLLM_MAX_NUM_SEQS:-8}",
      "--dtype", "auto"
    ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/v1/models', timeout=10)\""]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 180s
    restart: unless-stopped
    networks:
      - multimodal-net

  multimodal-worker:
    build:
      context: ./services/multimodal-worker
      dockerfile: Dockerfile.optimized
      cache_from:
        - multimodal-base:latest
        - multimodal-worker:latest
      args:
        BUILDKIT_INLINE_CACHE: 1
    container_name: multimodal-worker
    ports:
      - "8001:8001"
    environment:
      - CUDA_VISIBLE_DEVICES=0,1  # Allow access to both GPUs
      - NVIDIA_VISIBLE_DEVICES=0,1
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=${POSTGRES_DB:-multimodal}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD:-minioadmin}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
    volumes:
      - multimodal_cache:/app/cache
      - /tmp:/tmp
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    depends_on:
      qdrant:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - multimodal-net

  # AI Agents Backend Service
  ai-agents:
    build:
      context: ./services/ai-agents
      dockerfile: Dockerfile
    container_name: multimodal-ai-agents
    ports:
      - "8003:8003"
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=${POSTGRES_DB:-multimodal}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - LLM_BASE_URL=http://vllm:8000/v1
      - LLM_MODEL=${VLLM_MODEL:-microsoft/DialoGPT-medium}
      - MULTIMODAL_WORKER_URL=http://multimodal-worker:8001
      - RETRIEVAL_PROXY_URL=http://retrieval-proxy:8002
      - SEARCH_ENGINE_URL=http://search-engine:8004
      - MEMORY_SYSTEM_URL=http://memory-system:8005
      - USER_MANAGEMENT_URL=http://user-management:8006
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=2
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      redis:
        condition: service_healthy
      vllm:
        condition: service_healthy
      multimodal-worker:
        condition: service_healthy
      retrieval-proxy:
        condition: service_healthy
      search-engine:
        condition: service_healthy
      memory-system:
        condition: service_healthy
      user-management:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8003/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - multimodal-net

  # AI Agents Web Interface (Custom UI)
  ai-agents-web:
    build:
      context: .
      dockerfile: ./services/ai-agents/web/Dockerfile
    container_name: multimodal-ai-agents-web
    ports:
      - "3001:3000"
    volumes:
      - ./docs:/usr/share/nginx/html/docs:ro
    depends_on:
      ai-agents:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - multimodal-net

volumes:
  vllm_cache:
  multimodal_cache:

networks:
  multimodal-net:
    driver: bridge
