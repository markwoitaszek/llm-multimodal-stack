# vLLM Inference Server Environment Template
# This template contains environment variables for the vLLM inference service

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
VLLM_MODEL={{ vllm_model | default('microsoft/DialoGPT-medium') }}
VLLM_HOST={{ vllm_host | default('0.0.0.0') }}
VLLM_PORT={{ vllm_port | default('8000') }}
VLLM_GPU_MEMORY_UTILIZATION={{ vllm_gpu_memory_utilization | default('0.8') }}
VLLM_MAX_MODEL_LEN={{ vllm_max_model_len | default('1024') }}
VLLM_MAX_NUM_SEQS={{ vllm_max_num_seqs | default('8') }}

# =============================================================================
# GPU CONFIGURATION
# =============================================================================
CUDA_VISIBLE_DEVICES={{ cuda_visible_devices | default('0') }}
NVIDIA_VISIBLE_DEVICES={{ nvidia_visible_devices | default('0') }}

# =============================================================================
# API CONFIGURATION
# =============================================================================
VLLM_API_KEY={{ vault_vllm_api_key | default('changeme_in_production') }}
OPENAI_API_BASE_URL={{ openai_api_base_url | default('http://vllm:8000/v1') }}
LLM_BASE_URL={{ llm_base_url | default('http://vllm:8000/v1') }}

# =============================================================================
# CACHE CONFIGURATION
# =============================================================================
CACHE_TTL_MODEL_METADATA={{ cache_ttl_model_metadata | default('86400') }}
CACHE_TTL_EMBEDDINGS={{ cache_ttl_embeddings | default('86400') }}

# =============================================================================
# MODEL PATHS
# =============================================================================
MODELS_PATH={{ models_path | default('./models') }}