# Model Configuration Profiles
# Copy the desired profile to .env to use it

# =============================================================================
# SMALL MODELS (DialoGPT, GPT-2, etc.) - ~1-2GB model size
# =============================================================================
# VLLM_MODEL=microsoft/DialoGPT-large
# VLLM_GPU_MEMORY_UTILIZATION=0.2
# VLLM_MAX_MODEL_LEN=1024
# VLLM_MAX_NUM_SEQS=16

# =============================================================================
# MEDIUM MODELS (Llama-2 7B, Mistral 7B, etc.) - ~7-13GB model size
# =============================================================================
# VLLM_MODEL=microsoft/DialoGPT-large
# VLLM_GPU_MEMORY_UTILIZATION=0.6
# VLLM_MAX_MODEL_LEN=2048
# VLLM_MAX_NUM_SEQS=8

# =============================================================================
# LARGE MODELS (Llama-2 13B, CodeLlama 13B, etc.) - ~13-26GB model size
# =============================================================================
# VLLM_MODEL=microsoft/DialoGPT-large
# VLLM_GPU_MEMORY_UTILIZATION=0.8
# VLLM_MAX_MODEL_LEN=4096
# VLLM_MAX_NUM_SEQS=4

# =============================================================================
# RECOMMENDED MODELS FOR RTX 3090 (24GB)
# =============================================================================

# Option 1: Llama-2 7B (Good balance of performance and memory usage)
# VLLM_MODEL=meta-llama/Llama-2-7b-chat-hf
# VLLM_GPU_MEMORY_UTILIZATION=0.6
# VLLM_MAX_MODEL_LEN=4096
# VLLM_MAX_NUM_SEQS=8

# Option 2: Mistral 7B (Excellent performance, efficient memory usage)
# VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.1
# VLLM_GPU_MEMORY_UTILIZATION=0.5
# VLLM_MAX_MODEL_LEN=8192
# VLLM_MAX_NUM_SEQS=8

# Option 3: CodeLlama 7B (Great for code generation)
# VLLM_MODEL=codellama/CodeLlama-7b-Instruct-hf
# VLLM_GPU_MEMORY_UTILIZATION=0.6
# VLLM_MAX_MODEL_LEN=16384
# VLLM_MAX_NUM_SEQS=4

# Option 4: Llama-2 13B (Higher quality, more memory intensive)
# VLLM_MODEL=meta-llama/Llama-2-13b-chat-hf
# VLLM_GPU_MEMORY_UTILIZATION=0.8
# VLLM_MAX_MODEL_LEN=4096
# VLLM_MAX_NUM_SEQS=4

# =============================================================================
# CURRENT ACTIVE CONFIGURATION (uncomment one of the above)
# =============================================================================
VLLM_MODEL=microsoft/DialoGPT-large
VLLM_GPU_MEMORY_UTILIZATION=0.3
VLLM_MAX_MODEL_LEN=2048
VLLM_MAX_NUM_SEQS=8
