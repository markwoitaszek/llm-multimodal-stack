# Optimized Production Docker Compose Configuration
# Generated on: 2025-09-29T10:19:11.943811

version: '3.8'

services:
  # Optimized PostgreSQL
  postgres:
    image: postgres:16-alpine
    container_name: multimodal-postgres-optimized
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    command: >
      postgres
      -c shared_preload_libraries=pg_stat_statements
      -c pg_stat_statements.track=all
      -c max_connections=200
      -c shared_buffers=512MB
      -c effective_cache_size=2GB
      -c maintenance_work_mem=128MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c log_statement=all
      -c log_min_duration_statement=1000
    volumes:
      - postgres_optimized_data:/var/lib/postgresql/data
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - multimodal-net

  # Optimized Redis with clustering
  redis:
    image: redis:7-alpine
    container_name: multimodal-redis-optimized
    command: >
      redis-server
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 60
      --timeout 300
      --appendonly yes
      --appendfsync everysec
      --save 900 1
      --save 300 10
      --save 60 10000
    volumes:
      - redis_optimized_data:/data
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - multimodal-net

  # Optimized Qdrant
  qdrant:
    image: qdrant/qdrant:v1.12.0
    container_name: multimodal-qdrant-optimized
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__SERVICE__MAX_REQUEST_SIZE_MB=32
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS=4
      - QDRANT__STORAGE__OPTIMIZERS__MEMMAP_THRESHOLD_KB=200000
      - QDRANT__STORAGE__OPTIMIZERS__HNSW_CONFIG__M=16
      - QDRANT__STORAGE__OPTIMIZERS__HNSW_CONFIG__EF_CONSTRUCT=100
      - QDRANT__STORAGE__OPTIMIZERS__HNSW_CONFIG__FULL_SCAN_THRESHOLD=10000
    volumes:
      - qdrant_optimized_data:/qdrant/storage
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    healthcheck:
      test: ["CMD-SHELL", "pidof qdrant || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - multimodal-net

  # Optimized vLLM with performance tuning
  vllm:
    image: vllm/vllm-openai:latest
    container_name: multimodal-vllm-optimized
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - VLLM_MODEL=${VLLM_MODEL}
      - VLLM_HOST=0.0.0.0
      - VLLM_PORT=8000
      - VLLM_API_KEY=${VLLM_API_KEY}
    command: >
      --model ${VLLM_MODEL}
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization 0.85
      --max-model-len 4096
      --dtype auto
      --max-num-seqs 8
      --tensor-parallel-size 1
      --pipeline-parallel-size 1
      --api-key ${VLLM_API_KEY}
      --served-model-name gpt-3.5-turbo
      --chat-template ./chat_templates/chatml.jinja
    volumes:
      - vllm_optimized_cache:/root/.cache
      - ./models:/models:ro
    deploy:
      resources:
        limits:
          memory: 16G
          cpus: '8.0'
        reservations:
          memory: 8G
          cpus: '4.0'
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/v1/models', timeout=10)\""]
      interval: 60s
      timeout: 30s
      retries: 5
    restart: unless-stopped
    networks:
      - multimodal-net

  # Optimized LiteLLM with enhanced config
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: multimodal-litellm-optimized
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - LITELLM_SALT_KEY=${LITELLM_SALT_KEY}
      - LITELLM_LOG_LEVEL=INFO
      - LITELLM_DROP_PARAMS=true
      - LITELLM_SET_VERBOSE=false
      - LITELLM_MAX_BUDGET=1000
      - LITELLM_CACHE=True
    volumes:
      - ./configs/litellm_optimized.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--port", "4000", "--num_workers", "8"]
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'
    depends_on:
      vllm:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:4000/', timeout=5)\""]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - multimodal-net

  # Optimized Multimodal Worker with auto-scaling
  multimodal-worker:
    build:
      context: ./services/multimodal-worker
      dockerfile: Dockerfile
    container_name: multimodal-worker-optimized
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - LOG_LEVEL=INFO
      - WORKERS=4
      - MAX_CONCURRENT_REQUESTS=20
      - CACHE_TTL_SEARCH_RESULTS=3600
      - CACHE_TTL_MODEL_METADATA=86400
      - CACHE_TTL_EMBEDDINGS=86400
    volumes:
      - multimodal_optimized_cache:/app/cache
      - /tmp:/tmp
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 12G
          cpus: '6.0'
        reservations:
          memory: 6G
          cpus: '3.0'
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      qdrant:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - multimodal-net

  # Optimized Retrieval Proxy
  retrieval-proxy:
    build:
      context: ./services/retrieval-proxy
      dockerfile: Dockerfile
    container_name: retrieval-proxy-optimized
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=8
      - MAX_CONCURRENT_REQUESTS=40
      - CACHE_TTL_SEARCH_RESULTS=3600
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    depends_on:
      multimodal-worker:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:8002/health', timeout=5)\""]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - multimodal-net

  # Nginx Load Balancer with caching
  nginx:
    image: nginx:alpine
    container_name: multimodal-nginx-optimized
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./configs/nginx_optimized.conf:/etc/nginx/nginx.conf:ro
      - ./configs/ssl:/etc/nginx/certs:ro
      - nginx_cache:/var/cache/nginx
    depends_on:
      - litellm
      - multimodal-worker
      - retrieval-proxy
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - multimodal-net

volumes:
  postgres_optimized_data:
  redis_optimized_data:
  qdrant_optimized_data:
  vllm_optimized_cache:
  multimodal_optimized_cache:
  nginx_cache:

networks:
  multimodal-net:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/24
          gateway: 172.25.0.1
