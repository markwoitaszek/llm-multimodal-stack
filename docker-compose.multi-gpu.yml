# Multi-GPU Optimized Docker Compose Override
# Optimized for dual RTX 3090 with NVLink support
version: '3.8'

services:
  # vLLM with dual GPU tensor parallelism
  vllm:
    image: vllm/vllm-openai:latest
    environment:
      # Multi-GPU Configuration
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0,1}
      - NVIDIA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0,1}
      - CUDA_DEVICE_ORDER=${CUDA_DEVICE_ORDER:-PCI_BUS_ID}
      
      # vLLM Configuration
      - VLLM_MODEL=${VLLM_MODEL}
      - VLLM_HOST=${VLLM_HOST:-0.0.0.0}
      - VLLM_PORT=${VLLM_PORT:-8000}
      - VLLM_API_KEY=${VLLM_API_KEY}
    
    command: >
      --model ${VLLM_MODEL}
      --host ${VLLM_HOST:-0.0.0.0}
      --port ${VLLM_PORT:-8000}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION:-0.8}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-1024}
      --dtype auto
      --max-num-seqs ${VLLM_MAX_NUM_SEQS:-8}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE:-2}
      --pipeline-parallel-size ${VLLM_PIPELINE_PARALLEL_SIZE:-1}
      --api-key ${VLLM_API_KEY}
      --served-model-name gpt-3.5-turbo
      --chat-template ./chat_templates/chatml.jinja
      --disable-log-requests
      --disable-log-stats
    
    deploy:
      resources:
        limits:
          memory: 32G
          cpus: '16.0'
        reservations:
          memory: 16G
          cpus: '8.0'
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/v1/models', timeout=10)\""]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 120s

  # Multimodal Worker with dual GPU support
  multimodal-worker:
    environment:
      # Multi-GPU Configuration
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0,1}
      - NVIDIA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0,1}
      - CUDA_DEVICE_ORDER=${CUDA_DEVICE_ORDER:-PCI_BUS_ID}
      
      # Performance Configuration
      - WORKERS=${MULTIMODAL_WORKER_WORKERS:-4}
      - MAX_CONCURRENT_REQUESTS=20
      - CACHE_TTL_SEARCH_RESULTS=3600
      - CACHE_TTL_MODEL_METADATA=86400
      - CACHE_TTL_EMBEDDINGS=86400
    
    deploy:
      resources:
        limits:
          memory: 24G
          cpus: '12.0'
        reservations:
          memory: 12G
          cpus: '6.0'
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # LiteLLM with enhanced configuration for multi-GPU
  litellm:
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - LITELLM_SALT_KEY=${LITELLM_SALT_KEY}
      - LITELLM_LOG_LEVEL=INFO
      - LITELLM_DROP_PARAMS=true
      - LITELLM_SET_VERBOSE=false
      - LITELLM_MAX_BUDGET=1000
      - LITELLM_CACHE=True
      - LITELLM_NUM_WORKERS=${LITELLM_WORKERS:-8}
    
    command: ["--config", "/app/config.yaml", "--port", "4000", "--num_workers", "${LITELLM_WORKERS:-8}"]
    
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '4.0'
        reservations:
          memory: 2G
          cpus: '2.0'
    
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:4000/', timeout=5)\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Retrieval Proxy with enhanced workers
  retrieval-proxy:
    environment:
      - LOG_LEVEL=INFO
      - WORKERS=${RETRIEVAL_PROXY_WORKERS:-8}
      - MAX_CONCURRENT_REQUESTS=40
      - CACHE_TTL_SEARCH_RESULTS=3600
    
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:8002/health', timeout=5)\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

# Network optimization for multi-GPU
networks:
  multimodal-net:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/24
          gateway: 172.25.0.1
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "true"
      com.docker.network.bridge.host_binding_ipv4: "0.0.0.0"
